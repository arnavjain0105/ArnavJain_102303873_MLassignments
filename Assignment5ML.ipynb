{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GZ5sjf8sdxph"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(42)\n",
        "X_base=np.random.rand(200,1)*10\n",
        "X=np.hstack([X_base+np.random.randn(200,1)*0.5,X_base*0.9+np.random.randn(200,1)*0.4,X_base*1.1+np.random.randn(200,1)*0.3,X_base+np.random.randn(200,1)*0.2,X_base*1.05+np.random.randn(200,1)*0.3,X_base*0.95+np.random.randn(200,1)*0.2,X_base*1.2+np.random.randn(200,1)*0.5])\n",
        "true_w=np.array([3,-2,1,4,-1,2,0.5])\n",
        "true_b=10\n",
        "y=X.dot(true_w)+true_b+np.random.randn(200)*2\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "def ridge_regression_gd(X,y,lr=0.01,lam=0.1,epochs=1000):\n",
        "    m,n=X.shape\n",
        "    w=np.zeros(n)\n",
        "    b=0\n",
        "    X_mean=X.mean(axis=0)\n",
        "    X_std=X.std(axis=0)\n",
        "    X_norm=(X-X_mean)/X_std\n",
        "    cost_history=[]\n",
        "    for epoch in range(epochs):\n",
        "        y_pred=X_norm.dot(w)+b\n",
        "        error=y_pred-y\n",
        "        dw=(1/m)*(X_norm.T.dot(error))+(lam/m)*w\n",
        "        db=(1/m)*np.sum(error)\n",
        "        w-=lr*dw\n",
        "        b-=lr*db\n",
        "        cost=(1/(2*m))*np.sum(error**2)+(lam/(2*m))*np.sum(w**2)\n",
        "        if np.isnan(cost) or np.isinf(cost):break\n",
        "        cost_history.append(cost)\n",
        "    return w,b,cost_history,X_mean,X_std\n",
        "def predict(X,w,b,X_mean,X_std):\n",
        "    X_norm=(X-X_mean)/X_std\n",
        "    return X_norm.dot(w)+b\n",
        "learning_rates=[0.0001,0.001,0.01,0.1,1,10]\n",
        "lambdas=[1e-15,1e-10,1e-5,1e-3,0,1,10,20]\n",
        "best_config=None\n",
        "best_r2=-np.inf\n",
        "best_cost=np.inf\n",
        "for lr in learning_rates:\n",
        "    for lam in lambdas:\n",
        "        w,b,cost_history,X_mean,X_std=ridge_regression_gd(X_train,y_train,lr=lr,lam=lam,epochs=1000)\n",
        "        if len(cost_history)==0:continue\n",
        "        y_pred=predict(X_test,w,b,X_mean,X_std)\n",
        "        if np.any(np.isnan(y_pred)):continue\n",
        "        r2=r2_score(y_test,y_pred)\n",
        "        final_cost=cost_history[-1]\n",
        "        if r2>best_r2 or (r2==best_r2 and final_cost<best_cost):\n",
        "            best_config=(lr,lam,w,b)\n",
        "            best_r2=r2\n",
        "            best_cost=final_cost\n",
        "        print(f\"lr={lr}, lambda={lam:.1e}, Cost={final_cost:.4f}, R2={r2:.4f}\")\n",
        "best_lr,best_lambda,best_w,best_b=best_config\n",
        "print(\"\\nBest configuration found:\")\n",
        "print(f\"Learning Rate: {best_lr}\")\n",
        "print(f\"Regularization λ: {best_lambda}\")\n",
        "print(f\"Best R2 Score: {best_r2:.4f}\")\n",
        "print(f\"Final Cost: {best_cost:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBBErk3DeNoX",
        "outputId": "c84517b5-afb5-48d9-9cb1-1a4506c0e4b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr=0.0001, lambda=1.0e-15, Cost=997.8731, R2=-2.9682\n",
            "lr=0.0001, lambda=1.0e-10, Cost=997.8731, R2=-2.9682\n",
            "lr=0.0001, lambda=1.0e-05, Cost=997.8731, R2=-2.9682\n",
            "lr=0.0001, lambda=1.0e-03, Cost=997.8732, R2=-2.9682\n",
            "lr=0.0001, lambda=0.0e+00, Cost=997.8731, R2=-2.9682\n",
            "lr=0.0001, lambda=1.0e+00, Cost=997.9669, R2=-2.9684\n",
            "lr=0.0001, lambda=1.0e+01, Cost=998.8086, R2=-2.9698\n",
            "lr=0.0001, lambda=2.0e+01, Cost=999.7374, R2=-2.9714\n",
            "lr=0.001, lambda=1.0e-15, Cost=157.0229, R2=0.3749\n",
            "lr=0.001, lambda=1.0e-10, Cost=157.0229, R2=0.3749\n",
            "lr=0.001, lambda=1.0e-05, Cost=157.0229, R2=0.3749\n",
            "lr=0.001, lambda=1.0e-03, Cost=157.0231, R2=0.3749\n",
            "lr=0.001, lambda=0.0e+00, Cost=157.0229, R2=0.3749\n",
            "lr=0.001, lambda=1.0e+00, Cost=157.2563, R2=0.3749\n",
            "lr=0.001, lambda=1.0e+01, Cost=159.3380, R2=0.3742\n",
            "lr=0.001, lambda=2.0e+01, Cost=161.6119, R2=0.3733\n",
            "lr=0.01, lambda=1.0e-15, Cost=2.8089, R2=0.9888\n",
            "lr=0.01, lambda=1.0e-10, Cost=2.8089, R2=0.9888\n",
            "lr=0.01, lambda=1.0e-05, Cost=2.8089, R2=0.9888\n",
            "lr=0.01, lambda=1.0e-03, Cost=2.8092, R2=0.9888\n",
            "lr=0.01, lambda=0.0e+00, Cost=2.8089, R2=0.9888\n",
            "lr=0.01, lambda=1.0e+00, Cost=3.0664, R2=0.9887\n",
            "lr=0.01, lambda=1.0e+01, Cost=5.2978, R2=0.9881\n",
            "lr=0.01, lambda=2.0e+01, Cost=7.6558, R2=0.9875\n",
            "lr=0.1, lambda=1.0e-15, Cost=1.9871, R2=0.9930\n",
            "lr=0.1, lambda=1.0e-10, Cost=1.9871, R2=0.9930\n",
            "lr=0.1, lambda=1.0e-05, Cost=1.9871, R2=0.9930\n",
            "lr=0.1, lambda=1.0e-03, Cost=1.9877, R2=0.9930\n",
            "lr=0.1, lambda=0.0e+00, Cost=1.9871, R2=0.9930\n",
            "lr=0.1, lambda=1.0e+00, Cost=2.5082, R2=0.9924\n",
            "lr=0.1, lambda=1.0e+01, Cost=5.2391, R2=0.9892\n",
            "lr=0.1, lambda=2.0e+01, Cost=7.6461, R2=0.9879\n",
            "lr=1, lambda=1.0e-15, Cost=138217190450004532221457843711862261990311564054627456490880864698802708831731415553834461377983143248640789001431250700561001507809921575919790042651282606656981448488172653225601408959108568052158194495687653681861150431264639594732080523227513179444241447022848705624874097575422844741069389868619005952.0000, R2=-inf\n",
            "lr=1, lambda=1.0e-10, Cost=138217190456137901519556615533067948850239019821090383883338004205957455647081700219547363908387517540194686270238843954469898255669872539821173453139001208209712557920209375346512214564026227454475347600136861750810792185502855604755994594092617785327454688034731648284428624318119546873438240884987527168.0000, R2=-inf\n",
            "lr=1, lambda=1.0e-05, Cost=138217803784478698383419369712282648735026644646415488808801074986849015055296553607090796233260808022836589051616530806833964683880107305719865208467393065373025197126424413888771468738035898448498944718075313498872493847320292941802730182309522440571789534424063101798464545180767308294407684219228651520.0000, R2=-inf\n",
            "lr=1, lambda=1.0e-03, Cost=138278537282452380838572865434140492721943602177318030353811135574808449858630028692721887065047249592776704724044409041811495449462474197925375693612648956848179001059071658734970197123424248574671939559637308962874563417087513303579649385072946890838739554127159370166675479067379549424149867087773302784.0000, R2=-inf\n",
            "lr=1, lambda=0.0e+00, Cost=138217190450004532221457843711862261990311564054627456490880864698802708831731415553834461377983143248640789001431250700561001507809921575919790042651282606656981448488172653225601408959108568052158194495687653681861150431264639594732080523227513179444241447022848705624874097575422844741069389868619005952.0000, R2=-inf\n",
            "lr=1, lambda=1.0e+00, Cost=215277732216351200817438851586436234786623396763114785063864124794546768029233832317647800718675551601079532687195306409976414360896984732486436308027681794055164256915778222708701814981470105977115384214884618183113067818695566054653984022171847124997444349359828397845007340973964461483703919121912561664.0000, R2=-inf\n",
            "lr=1, lambda=1.0e+01, Cost=308219503770122796005321678464306668709816934652515906538395488690084354281277936371782225260890593795497112213848529324410909890905784317674003632381804700236087684648714209763279106298804073179750676483413167107306150103113877158063913033657275364466469508212673776910114207377532148023271054034302140416.0000, R2=-inf\n",
            "lr=1, lambda=2.0e+01, Cost=599011980768041244289256926643595783173599724299582607489204147050834060468599875502514002827143224306556284970827425152485662305885953995597611723074516532684529108612580145312969007276489173759465934356660815701319528696364521926733774439189169474218175922450694681937125419268205569113559930152222195712.0000, R2=-inf\n",
            "lr=10, lambda=1.0e-15, Cost=1395887352791756980808806454902459427580982064654196914198634261757126285412320401721440195575203954246155630150292226164110351247332420172899635592409085517663218339031005509742453658255159802261648851879439162197916928992487795745642542128110675794839882565554262515892325813212297175738657408813105152.0000, R2=-inf\n",
            "lr=10, lambda=1.0e-10, Cost=1395887353398815431158626616365516194761699127011280701314924768904619275287874068517774861308964770485256710158981528704562214552546146846232507890322100013508259253315060824104851636269475049565379348448573375749784658730605156841035526532719370877487168188965457430950511037270525251645011532334497792.0000, R2=-inf\n",
            "lr=10, lambda=1.0e-05, Cost=1395948059142751916538383619082705755489718389653692923180770310431908739823272996457044160101978985798855987034391059108350837938436513239082565718939106566263534172731440868338448369302761483804628977193563597490030466553130402141248070632360778720754908292379137485395837890787342304650500680116600832.0000, R2=-inf\n",
            "lr=10, lambda=1.0e-03, Cost=1401958875758711346714944324075390165906781189640223497042035497030516722642422280888973372933330373286500112872153876450521305047124973092134583433028439051744355285056550335242926766400034147059904119405487421822983635396207028683354183749705150079677885370863505437436535283087242526189898129330929664.0000, R2=-inf\n",
            "lr=10, lambda=0.0e+00, Cost=1395887352791751194528612186153005467760189263761137331438937564721407207592861431533155053009705615287210262706700811139316338848435686244017528128071995361983261153393037663046319309099298316480290904660085452723158300706575339494818773924903676757936485880855929899422411470047503595902932101114101760.0000, R2=-inf\n",
            "lr=10, lambda=1.0e+00, Cost=8433296362191760176134437976508530612679045796852907005533060666530078903579265175739628327345600323422058788836755671389915376144023441927006709409172400907116204781230707035236033159228836045039805174191891766486959969675498582955742564268246251865230792864107188089222694007427608264832467188234321920.0000, R2=-inf\n",
            "lr=10, lambda=1.0e+01, Cost=267983411057566676918769781727646184509412546680104607031389446080741779103311360391692522763656962793178886576092530702059230953677911238694420316520925448794936609585363973381168970909171463035825263563951147521656391460270195248471304684437312601945231417377553765161707978629383677678043320399656124416.0000, R2=-inf\n",
            "lr=10, lambda=2.0e+01, Cost=2334049732707698189545582132111930920622216345510997027451057583429420540743365474423658267943403890399873487132019136715862934177959485445491726510631244155136898283497188922484879262516819776598782613035599596461221734387180235926305263339332167043552102135955201959656709186124902289082351735873879408640.0000, R2=-inf\n",
            "\n",
            "Best configuration found:\n",
            "Learning Rate: 0.1\n",
            "Regularization λ: 0\n",
            "Best R2 Score: 0.9930\n",
            "Final Cost: 1.9871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-4271665354.py:26: RuntimeWarning: overflow encountered in square\n",
            "  cost=(1/(2*m))*np.sum(error**2)+(lam/(2*m))*np.sum(w**2)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-4271665354.py:26: RuntimeWarning: overflow encountered in square\n",
            "  cost=(1/(2*m))*np.sum(error**2)+(lam/(2*m))*np.sum(w**2)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-4271665354.py:26: RuntimeWarning: overflow encountered in square\n",
            "  cost=(1/(2*m))*np.sum(error**2)+(lam/(2*m))*np.sum(w**2)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-4271665354.py:26: RuntimeWarning: overflow encountered in square\n",
            "  cost=(1/(2*m))*np.sum(error**2)+(lam/(2*m))*np.sum(w**2)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-4271665354.py:26: RuntimeWarning: overflow encountered in square\n",
            "  cost=(1/(2*m))*np.sum(error**2)+(lam/(2*m))*np.sum(w**2)\n",
            "/tmp/ipython-input-4271665354.py:26: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  cost=(1/(2*m))*np.sum(error**2)+(lam/(2*m))*np.sum(w**2)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-4271665354.py:26: RuntimeWarning: overflow encountered in square\n",
            "  cost=(1/(2*m))*np.sum(error**2)+(lam/(2*m))*np.sum(w**2)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/tmp/ipython-input-4271665354.py:26: RuntimeWarning: overflow encountered in square\n",
            "  cost=(1/(2*m))*np.sum(error**2)+(lam/(2*m))*np.sum(w**2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n",
            "/tmp/ipython-input-4271665354.py:26: RuntimeWarning: overflow encountered in square\n",
            "  cost=(1/(2*m))*np.sum(error**2)+(lam/(2*m))*np.sum(w**2)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
            "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "iris=load_iris()\n",
        "X=iris.data\n",
        "y=iris.target\n",
        "sc=StandardScaler()\n",
        "X=sc.fit_transform(X)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "def sigmoid(z):return 1/(1+np.exp(-z))\n",
        "def logistic_regression(X,y,lr=0.1,epochs=1000):\n",
        "    m,n=X.shape\n",
        "    w=np.zeros(n)\n",
        "    b=0\n",
        "    cost_history=[]\n",
        "    for i in range(epochs):\n",
        "        z=X.dot(w)+b\n",
        "        y_pred=sigmoid(z)\n",
        "        error=y_pred-y\n",
        "        dw=(1/m)*X.T.dot(error)\n",
        "        db=(1/m)*np.sum(error)\n",
        "        w-=lr*dw\n",
        "        b-=lr*db\n",
        "        cost=-(1/m)*np.sum(y*np.log(y_pred+1e-9)+(1-y)*np.log(1-y_pred+1e-9))\n",
        "        cost_history.append(cost)\n",
        "    return w,b,cost_history\n",
        "def predict_prob(X,w,b):return sigmoid(X.dot(w)+b)\n",
        "classes=np.unique(y_train)\n",
        "weights=[]\n",
        "biases=[]\n",
        "for c in classes:\n",
        "    y_binary=(y_train==c).astype(int)\n",
        "    w,b,_=logistic_regression(X_train,y_binary,lr=0.1,epochs=2000)\n",
        "    weights.append(w)\n",
        "    biases.append(b)\n",
        "weights=np.array(weights)\n",
        "biases=np.array(biases)\n",
        "def predict(X,weights,biases):\n",
        "    probs=[predict_prob(X,weights[i],biases[i]) for i in range(len(weights))]\n",
        "    probs=np.array(probs).T\n",
        "    return np.argmax(probs,axis=1)\n",
        "y_pred=predict(X_test,weights,biases)\n",
        "acc=accuracy_score(y_test,y_pred)\n",
        "print(\"Accuracy:\",acc)\n",
        "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
        "print(\"Classification Report:\\n\",classification_report(y_test,y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_noqBzsoF7t",
        "outputId": "387d8c91-3f32-49fe-9aa7-dedf2d5551df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      0.89      0.94         9\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ]
    }
  ]
}